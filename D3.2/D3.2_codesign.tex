\documentclass[a4paper,12pt]{article}

% Import the deliverable package from common directory
\usepackage{../common/deliverable}

% Tell LaTeX where to find graphics files
\graphicspath{{../common/logos/}{./figures/}{../}}

\usepackage{xspace}
\usepackage{lipsum}
\usepackage{listings}
\usepackage{xcolor} % for custom colors

% Small font size in lstlistings
\lstset{basicstyle=\ttfamily\small}

% Define a custom DSL (keywords of your choice)
\lstdefinelanguage{TensorIR}{
    morekeywords={alloca,gemm,gemv,fuse,subview},
    morecomment=[l]{\;},
}


% Set the deliverable number (without the D prefix, it's added automatically)
\setdeliverableNumber{3.2}

% Begin document
\begin{document}

% Create the title page with the title as argument
\maketitlepage{Co-Design and Energy Efficiency Report I}

\newpage

% Main Table using the new environment and command
\begin{deliverableTable}
    \tableEntry{Deliverable title}{Co-Design and Energy Efficiency Report}
    \tableEntry{Deliverable number}{D3.2}
    \tableEntry{Deliverable version}{0.1}
    \tableEntry{Date of delivery}{31 August 2025}
    \tableEntry{Actual date of delivery}{[Actual date]}
    \tableEntry{Nature of deliverable}{Report}
    \tableEntry{Dissemination level}{Public}
    \tableEntry{Work Package}{WP3}
    \tableEntry{Partner responsible}{BADW-LRZ}
\end{deliverableTable}

% Abstract and Keywords Section
\begin{deliverableTable}
    \tableEntry{Abstract}{\lipsum[1][1-5]}
    \tableEntry{Keywords}{performance; benchmarks; gpu-programming; Keyword 4; Keyword 5}
\end{deliverableTable}

\newpage

\begin{documentControl}
    \addVersion{0.1}{[Date]}{Ivan Pribec}{Initial draft}
    \addVersion{0.2}{[Date]}{[Author name]}{[Description of changes]}
    \addVersion{0.3}{[Date]}{[Author name]}{[Description of changes]}
    \addVersion{1.0}{[Date]}{[Author name]}{Final version}
\end{documentControl}

\subsection*{{Approval Details}}
Approved by: [Name] \\
Approval Date: [Date]

\subsection*{{Distribution List}}
\begin{itemize}
    \item [] - Project Coordinators (PCs)
    \item [] - Work Package Leaders (WPLs)
    \item [] - Steering Committee (SC)
    \item [] - European Commission (EC)
\end{itemize}

\vspace*{2cm}

\disclaimer

\newpage

\tableofcontents % Automatically generated and hyperlinked Table of Contents

\newpage

\section{{Introduction}}

This report described advances in co-design, technology exploitation and energy efficiency.

\begin{itemize}
    \item Investigate novel hardware and software solutions to leverage the latest advancements in high-performance computing
to achieve optimal performance
    \item Identify and evaluate emerging technologies that can be exploited to enhance the capabilities of the exascale computing
system.
    \item Explore and implement techniques to optimize energy consumption and power efficiency in the exascale computing
infrastructure.
    \item Identify and address bottlenecks in the system to improve overall computational efficiency.
\end{itemize}

\subsection{{Purpose of the Document}}

The objective of this document is to describe our ongoing co-design activies
aimed at achieving optimal performance, energy efficiency and technology exploitation
on novel hardware architectures. 


For performance optimization purposes we have focused on two families of kernels, the CEED Bakeoff Problems and Streaming Kernels, that serve as a suitable proxy for the algorithmic core of simulation codes using high-order finite elements.

The CEED Bakeoff problems focus on the fast evaluation of the basic operators, namely bilinear forms, on hexahedral grids using sum factorization techniques. 
Both scalar and vector PDE problems are included in the benchmark, so far our efforts have been focused on the scalar problems.
Each of the Bakeoff Problems (BP) has an associated Benchmark Kernel (BK) that evaluates a particular bilinear form. 
For high-order finite elements, the BKs rank highly in terms of arithmetic intensity.
The efficient evaluation of these kernels is paramount to obtain good performance. 

The Streaming Kernels capture a second group of low intensity operations, where memory movement is dominant.
The kernels generally take one or two long vectors as arguments. 
The first four kernels included are: copy, scaled sum of vectors (AXPBY), squared norm (NRM2), and inner product of two vectors (DOT). 
The fifth kernel performs a fused scaled sum and dot product. 
The remaining two kernels are the gather and scatter kernels that involve packing and unpacking the degrees of freedom
associated with a given finite element mesh into a linear vector.
The gather and scatter kernels are not addressed in this report.


\begin{itemize}
\item CUDA Support: Enabling programming support for NVIDIA GPUs using the CUDA parallel computing model and
supporting compatibility with AMDs HIP framework.
\item SYCL Integration: Incorporating SYCL (Standard C++ for heterogeneous computing) for programming heterogeneous
systems using C++. Porting from and compatibility with CUDA.
\item Using OpenMP and/or OpenACC offloading techniques and ensuring their compatibility on different
platforms.
\item Employing parallel programming techniques like SIMD (Single Instruction, Multiple Data) for efficient accelerator
and CPU utilization.
\item We will support programming approaches and frameworks such as Kokkos (already supported by deal.II) and RAJA
that supporting a range of accelerator programming paradigms ensure flexibility and compatibility with different
hardware architectures, enabling developers to harness the full potential of accelerators in their applications. This avoids maintaining paradigm-specific code branches in parallel.
\end{itemize}

\newpage

\section{CEED Bakeoff Problems}

To study the performance of high-order finite element we focus on a representative
set of problems originating from the CEED project \cite{}, designed to test and compare
the performance of high-order codes.

The CEED benchmarks\footnote{\url{https://ceed.exascaleproject.org/bps/}} include the following problems,
\begin{itemize}
    \item BP1: scalar PCG with mass matrix, $q = p+2$
    \item BP2: vector PCG with mass matrix, $q = p+2$
    \item BP3: scalar PCG with stiffness matrix, $q = p+2$
    \item BP4: vector PCG with stiffness matrix, $q = p+2$
    \item BP5: scalar PCG with stiffness matrix, $q = p+1$
    \item BP6: scalar PCG with stiffness matrix, $q = p+1$
\end{itemize}
In parallel to the set of benchmark problems, are the benchmark kernels,
\begin{itemize}
    \item BP1: scalar PCG with mass matrix, $q = p+2$
    \item BP2: vector PCG with mass matrix, $q = p+2$
    \item BP3: scalar PCG with stiffness matrix, $q = p+2$
    \item BP4: vector PCG with stiffness matrix, $q = p+2$
    \item BP5: scalar PCG with stiffness matrix, $q = p+1$
    \item BP6: scalar PCG with stiffness matrix, $q = p+1$
\end{itemize}
which exclude the scatter/gather steps that are necessary when 
the problem is partitioned across devices.

\subsection{CUDA Support}
\subsection{SYCL Integration}
\subsection{OpenMP Offloading}
\subsection{Kokkos Portability Framework}
\subsection{Tiny Tensor Compiler}

\label{sec:tinytc}

The Tiny Tensor Compiler (TinyTC) is an open-source tensor compiler developed by Intel for efficient execution of tensor computations on CPUs and GPUs. TinyTC compiles programs written in a domain-specific tensor language into OpenCL-C or SPIR-V, supporting runtime environments such as OpenCL, Level Zero, and SYCL. C and C++ APIs are available. TinyTC assumes a batched execution model, where each kernel is executed by a work-group with concurrent work-items, and the compiler maps tensor operations to efficient GPU instructions, including cooperative GEMMs and subgroup vectorization. Further details of the execution model and available instructions can be found in the online documentation \footnote{\url{https://intel.github.io/tiny-tensor-compiler/manual/tensor-ir.html}, accessed on 25.08.2028}

Domain-specific languages (DSLs) provide abstraction and performance portability by expressing operations in a form closer to the mathematical problem rather than low-level loops. In recent years, DSLs such as FreeFem++ or FEniCS UML allow variational forms to be compiled into optimized C or LLVM IR. Similarly, TinyTC aims to represent tensor operations, representing multi-dimensional contractions, fusions, and GEMMs in a dedicated tensor language, but remains unaware of finite element specifics.

Our first test of TinyTC aims to accelerate the finite element mass matrix evaluation (BK1) using sum factorization. As a first demonstration of tensor language we present the BK1 kernel using algorithm presented by Åšwirydowicz et al. (2019), see Listing \ref{lst:tinytc}, where each tensor product is implemented using the loop over GEMM approach. For simplicity the implementation uses four temporary arrays \texttt{wsp0}-\texttt{wsp4} are allocated in local work-group storage.

\begin{lstlisting}[language=TensorIR,caption={BK1 using Tensor IR},basicstyle=\ttfamily\tiny,
                   label={lst:tinytc}]
func @sum_factorization(%basis0: memref<f32x3x4>,
                        %basis1: memref<f32x3x4>,
                        %basis2: memref<f32x3x4>,
                        %JxW:    memref<f32x4x4x4x?>,
                        %in:     memref<f32x3x3x3x?>,
                        %out:    memref<f32x3x3x3x?>) {

    %gid = group_id

    %wsp0 = alloca -> memref<f32x3x3x3>; // Reserve temporary memory
    %wsp1 = alloca -> memref<f32x3x3x4>; // Reserve temporary memory
    %wsp2 = alloca -> memref<f32x3x4x4>; // Reserve temporary memory
    %wsp3 = alloca -> memref<f32x4x4x4>; // Reserve temporary memory
    %wsp4 = alloca -> memref<f32x4x4x4>;

    %J_e = subview %JxW[:,:,:,%gid] : memref<f32x4x4x4x?>;
    %in_e = subview %in[:,:,:,%gid] : memref<f32x3x3x3x?>;
    %out_e = subview %out[:,:,:,%gid] : memref<f32x3x3x3x?>;

    for %j1 = 0, 3 { ; Batch of GEMMs
        %tmp = subview %in_e[:,%j1,:] : memref<f32x3x3x3>
        %res = subview %wsp1[:,%j1,:] : memref<f32x3x3x4>
        gemm.n.n 1.0, %tmp, %basis0, 0.0, %res :
            f32, memref<f32x3x3,strided<1,9>>, memref<f32x3x4>, f32, memref<f32x3x4,strided<1,9>>;
    }

    for %j2 = 0, 4 { ; Batch of GEMMs
        %tmp = subview %wsp1[:,:,%j2] : memref<f32x3x3x4>
        %res = subview %wsp2[:,%j2,:] : memref<f32x3x4x4>
        gemm.n.n 1.0, %tmp, %basis1, 0.0, %res :
            f32, memref<f32x3x3>, memref<f32x3x4>, f32, memref<f32x3x4,strided<1,12>>
    }

    for %j3 = 0, 4 { ; Batch of GEMV
        for %p3 = 0, 4 {
            %tmp = subview %wsp2[:,%p3,%j3] : memref<f32x3x4x4>
            %res = subview %wsp3[:,%j3,%p3] : memref<f32x4x4x4>
            gemv.t 1.0, %basis2, %tmp, 0.0, %res :
                f32, memref<f32x3x4>, memref<f32x3>, f32, memref<f32x4> 
        }
    }    

    %flat1 = fuse %J_e[0,2] : memref<f32x4x4x4>
    %flat2 = fuse %wsp3[0,2] : memref<f32x4x4x4>
    %flat3 = fuse %wsp4[0,2] : memref<f32x4x4x4>

    hadamard 1.0, %flat1, %flat2, 0.0, %flat3 :
        f32, memref<f32x64>, memref<f32x64>, f32, memref<f32x64>

    for %q6 = 0, 4 {                               ; Step 6
        for %p6 = 0, 4 {
            %tmp = subview %wsp4[:,%q6,%p6] : memref<f32x4x4x4>
            %res = subview %wsp2[:,%p6,%q6] : memref<f32x3x4x4>
            gemv.n 1.0, %basis2, %tmp, 0.0, %res :
                f32, memref<f32x3x4>, memref<f32x4>, f32, memref<f32x3> 
        }
    }

    for %p7 = 0, 4 {
        %tmp = subview %wsp2[:,%p7,:] : memref<f32x3x4x4>
        %res = subview %wsp1[:,:,%p7] : memref<f32x3x3x4>
        gemm.n.t 1.0, %tmp, %basis1, 0.0, %res :
            f32, memref<f32x3x4,strided<1,12>>,memref<f32x3x4>, f32, memref<f32x3x3>
    }

    for %j8 = 0, 3 {
        %tmp = subview %wsp1[:,%j8,:] : memref<f32x3x3x4>
        %res = subview %out_e[:,%j8,:] : memref<f32x3x3x3>
        gemm.n.t 1.0, %tmp, %basis0, 0.0, %res :
            f32, memref<f32x3x4,strided<1,9>>, memref<f32x3x4>, f32, memref<f32x3x3,strided<1,9>>
    }
    
}
\end{lstlisting}

As the \texttt{alloca} instruction reserves temporary memory in shared local memory, TinyTC automatically inserts thread barriers between each of the six GEMM passes to ensure functional correctness. The kernel shown above only attained 10 GDoF/s on the Intel Ponte Vecchio GPU. One of several optimizations possible is fusing dimensions, to obtain larger GEMMs with a more favourable operation count. For instance in step 1 (the \texttt{j1}-loop), the loop over GEMMs can be rewritten as a single GEMM by fusing dimensions (Listing \ref{lst:tincytc_fuse}).

\begin{lstlisting}[language=TensorIR,caption={BK1 using Tensor IR},basicstyle=\ttfamily\small,
                   label={lst:tinytc_fuse}]
    %tmp1 = fuse %in_e[0,1] : memref<f32x9x3>
    %res1 = fuse %wsp1[0,1] : memref<f32x9x4,local>    
    gemm.n.n %c1, %tmp1, %basis0, %c0, %res1
\end{lstlisting}

Other GEMM steps can be fused in a similar manner, however this only delivered a few percent improvement. Upon consultation with the TinyTC author it became apparent a different strategy would be needed to obtain performant execution and optimal SIMD utilization of the PVC architecture.
In collaboration with Intel, we are currently investigating the application of index fusion, batch vectorization and memory layout transformations to make guarantee optimal register and SIMD usage.

Despite these initial challenges, the use of a specialized tensor DSL offers numerous potential benefits including,
\begin{itemize}
\item Readability and maintenance: kernels are expressed in a high-level tensor operations rather than nested loops
\item Hardware portability: TinyTC maps tensor operations to different architectures without rewriting kernels. Currently, this is portability is limited to Intel devices.
\item Runtime specialization: kernels written in IR are ingested as strings for just-in-time compilation, allowing constant loop bounds for optimal looping and GEMM optimizations
\item Optimized memory and SIMD usage: temporary arrays are automatically placed in shared local memory; cooperative matrix instructions can reduce synchronization overhead.
\end{itemize}

\section{Streaming Kernels}

Iterative linear system solvers make heavy use of low arithmetic intensity operations, so called streaming operations.
When matrix-free high-order FEM schemes are combined with iterative solvers. 
Kronbichler et al. \cite{} have shown previously on an example of the conjugate gradient method, that above a certain problem size, the streaming operations contribute a significant chunk of the run-time.

In this section we present some preliminary results on the performance and portability of
streaming operations implemented using different frameworks on CPU and GPU architectures.

\subsection{Kokkos}



\subsection{OpenMP Offloading}

Since their introduction in OpenMP 4.5, offloading has become an integral component
of the OpenMP programming framework.
Adding OpenMP directives offloading to an existing code can be quite straightforward,
assuming the right data structures and loop patterns are already in place.
When using OpenMP for GPU offloading it is important to distinguish between two
categories of directives, memory movement and work sharing.

To provide fine-grained control over the hierarchical parallelism in terms of 
teams and threads. The two main directives from this point of view are, `omp teams` and `omp parallel`, which can also be used to program in a SPMD-like mode (also called "me" mode). Typically however, there directives are combined with work-sharing
constructs like `distribute`, `for` to control the parallelization of (nested) loops.
A very useful work-sharing construct is the `loop` directive, which provides 
a descriptive form of parallelism, which is slightly less flexible.
The advantage is the precise assignment of the loop iteration space to teams and threads is left to the compiler. 
As explained by Deakin \& Mattson, this kind of descriptiveness provides
superior performance portability, assuming the compilers succeeds to auto-parallelize
the loop well.

\begin{lstlisting}[language=C++,caption={Simple C++ hello world},label={lst:cpp-hello}]
#pragma omp target \
    map(to: basis0[:nm0*nq0], basis1[:nm1*nq1], basis2[:nm2*nq2]) \
    map(to: in[:nelmt*nm0*nm1*nm2], JxW[:nelmt*nq0*nq1*nq2]) \
    map(from: out[:nelmt*nm0*nm1*nm2])
#pragma omp teams loop
for(size_t e = 0; e < nelmt; ++e) {
    /* ... sum factorization ... */
}
\end{lstlisting}

In figure X we show the parallelization of the BK1 algortihm using the \texttt{omp loop} directives. First we open a `target` region, instructing the compiler to generate
device code for the upcoming region/scope. The `map` clauses are used to communicate the
array sizes and avoid unnecessary memory movement.
For this particular kernel the simpler `omp loop` directive would also work, but here we wanted
to emphasize the assigment of elements to individual teams.

The OpenMP model encourages programmers to focus on correctness of the
(sequential) algorithm first.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.8\textwidth]{pvc_openmp} % replace with your filename
  \caption{BK1 using OpenMP, Intel Max Series 1550 GPU (Ponte Vecchio).}
  \label{fig:pvc_openmp}
\end{figure}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.8\textwidth]{pvc_opencl} % replace with your filename
  \caption{BK1 using OpenCL and 3D Simple Map work-item strategy, Intel Max Series 1550 GPU (Ponte Vecchio).}
  \label{fig:pvc_opencl}
\end{figure}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.8\textwidth]{pvc_kokkos} % replace with your filename
  \caption{BK1 using Kokkos with SYCL backend, Intel Max Series 1550 GPU (Ponte Vecchio).}
  \label{fig:pvc_kokkos}
\end{figure}

%
% Figures (q = 4)
%

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.8\textwidth]{gh200_templated} % replace with your filename
  \caption{BK1 performance for templated kernel, $q = 4$, Nvidia GH200 (Hopper).}
  \label{fig:gh200_templated}
\end{figure}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.8\textwidth]{gh200_variable} % replace with your filename
  \caption{BK1 performance for variable kernel, $q = 4$, Nvidia GH200 (Hopper).}
  \label{fig:gh200_variable}
\end{figure}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.8\textwidth]{pvc_opencl_q4_static} % replace with your filename
  \caption{BK1 performance for templated kernel, $q = 4$, Intel Max Series 1550 GPU (Ponte Vecchio).}
  \label{fig:pvc_static}
\end{figure}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.8\textwidth]{pvc_opencl_q4_dynamic} % replace with your filename
  \caption{BK1 performance for variable kernel, $q = 4$, Intel Max Series 1550 GPU (Ponte Vecchio).}
  \label{fig:pvc_dynamic}
\end{figure}


\section{Energy Efficiency}

Implement energy measurement strategies for the main algorithmic ingredients

FIXME: cite Salvatore's work, energy-meter 

\section{Algorithmic development}


\section{Next steps}

\begin{itemize}
    \item TinyTC kernel optimization using fusion and cross-element vectoritation
    \item Use of advanced multigrid solvers and preconditioners
\end{itemize}


\section{Hardware systems}

\begin{center}
    \begin{table}[h!]
    \small
    \caption{GPUs used for evaluation}
    \renewcommand{\arraystretch}{1.25}
    \label{tab:example_table}
    \begin{tabular}{|l|l|l|c|}
    \hline
    \textbf{Column 1} & \textbf{Ampere} & \textbf{Hopper} & \textbf{Ponte Vecchio} \\
    \hline
    execution units & Description & Category & Value \\
    base frequency & Description & Category & Value \\
    SIMT/SIMD width & Description & Category & Value \\
    last level cache & Description & Category & Value \\
    memory bandwidth & Description & Category & Value \\
    \hline
    \end{tabular}
    \end{table}
\end{center}


\end{document}
