\documentclass[a4paper,12pt]{article}

% Import the deliverable package from common directory
\usepackage{../common/deliverable}

% Tell LaTeX where to find graphics files
\graphicspath{{../common/logos/}{./figures/}{../}}

\usepackage{xspace}
\usepackage{lipsum}

% Set the deliverable number (without the D prefix, it's added automatically)
\setdeliverableNumber{1.4}

% Begin document
\begin{document}

% Create the title page with the title as argument
\maketitlepage{Enhancements in Pre-Exascale Modules of deal.II}

\newpage

% Main Table using the new environment and command
\begin{deliverableTable}
    \tableEntry{Deliverable title}{Enhancements in Pre-Exascale Modules}
    \tableEntry{Deliverable number}{D1.4}
    \tableEntry{Deliverable version}{1.0}
    \tableEntry{Date of delivery}{31 August 2025}
    \tableEntry{Actual date of delivery}{31 August 2025}
    \tableEntry{Nature of deliverable}{Report}
    \tableEntry{Dissemination level}{Public}
    \tableEntry{Work Package}{WP1}
    \tableEntry{Partner responsible}{UNIPI}
\end{deliverableTable}

% Abstract and Keywords Section
\begin{deliverableTable}
    \tableEntry{Abstract}{ This report documents the progress achieved during
        the second semester of the dealii-X project (Months 7-11) in enhancing the
        deal.II finite element library towards exascale readiness. Building upon
        the foundations laid in D1.2, this deliverable focuses on the
        consolidation of the new features, their integration into real-world
        biomedical applications, and preliminary scaling tests on EuroHPC
        pre-exascale systems. Major achievements include significant
        improvements in GPU matrix-free solvers, the stabilization and extension
        of the generalized interface for coupling operators, the maturation of
        the polygonal discretization module, and the integration of external
        solver technologies (PSCToolkit and MUMPS) into the deal.II workflow. }
        \tableEntry{Keywords}{Mesh handling; Polygonal discretization; Non-matching methods and preconditioning}
\end{deliverableTable}

\newpage

\begin{documentControl}
    \addVersion{0.1}{1 August 2025}{Luca Heltai}{Initial draft}
    \addVersion{0.2}{27 August 2025}{Luca Heltai}{Collection of contributions}
    \addVersion{0.3}{28 August 2025}{Marco Feder}{Add preconditioner section}
    \addVersion{1.0}{31 August 2025}{Luca Heltai}{Final version}
\end{documentControl}

\subsection*{{Approval Details}}
Approved by: Martin Kronbichler \\
Approval Date: 31 August 2025

\subsection*{{Distribution List}}
\begin{itemize}
    \item [] - Project Coordinators (PCs)
    \item [] - Work Package Leaders (WPLs)
    \item [] - Steering Committee (SC)
    \item [] - European Commission (EC)
\end{itemize}

\vspace*{2cm}

\disclaimer

\newpage

\tableofcontents % Automatically generated and hyperlinked Table of Contents

\newpage

\section{{Introduction}}

The goal of Work Package 1 (WP1) is to strengthen the numerical and software backbone of the dealii-X Centre of Excellence by extending the capabilities of the deal.II library, enabling the exascale transition of digital twins for human organs. Deliverable D1.4 provides a consolidated report on the technical progress, following the preliminary results described in D1.2, and emphasizes integration, benchmarking, and readiness for multiphysics biomedical applications.

\section{{Purpose of the Document}}

The purpose of this document is to provide a comprehensive overview of the advancements made in the deal.II library as part of the dealii-X project. It aims to inform stakeholders about the current state of the library, the challenges encountered, and the strategies employed to overcome them.

\section{Improvements for Exascale Readiness (WP1.1)}
\label{sec:section2}

The 9.7 release of deal.II has introduced several improvements directly aligned
with WP1.1 on GPU-enabled matrix-free solvers and exascale readiness.  
Key advances concern the matrix-free infrastructure, which was extended with:


\begin{itemize}
  \item Support for multi-component and vector-valued finite elements in the
        matrix-free evaluation routines. This substantially broadens the class
        of multiphysics applications (e.g.\ poromechanics in brain and liver)
        that can be ported to GPUs efficiently.
  \item Portability layer \texttt{Portable::MatrixFree} built on top of
        \texttt{kokkos}, ensuring code can target NVIDIA, AMD, and Intel GPUs with the
        same interface. The release consolidates temporary storage structures
        into new \texttt{DeviceVector} and \texttt{DeviceBlockVector} classes,
        simplifying GPU memory management.
  \item Performance optimizations to sum factorization kernels, yielding
        measurable speedups especially on NVIDIA A100/H100 hardware.
  \item Extended ghost cell handling and lexicographic patch smoothers for
        geometric multigrid preconditioners, crucial to scaling implicit solvers
        to $10^{11}$-$10^{12}$ DoFs on EuroHPC machines.
\end{itemize}

In addition, new coarse-level solver strategies were enabled by the\\
\texttt{TensorProductMatrixCreator} for diagonalization-based inverses.  
These improvements directly support the objective of saturating node-level
bandwidth on GPUs and minimizing data transfer overheads, as required for
exascale digital twin workloads.


\section{Improvement of pre-exascale modules of the deal.II library (WP1.2)}

In addition to the enhancements reported in Sections~\ref{sec:section2} and~\ref{sec:section3},
further work has been carried out on the integration of external modules that are
essential for the pre-exascale readiness of deal.II and for supporting the
targeted biomedical applications.

\subsection{Integration of the Gmsh module}

A dedicated module for interfacing with \texttt{Gmsh} is currently under
review in the official deal.II repository
(\url{https://github.com/dealii/dealii/pull/18759}).  
The new contribution introduces a function to read \emph{partitioned} mesh files
generated by Gmsh directly into a \texttt{parallel::fullydistributed::Triangulation}.
This development is critical for large-scale applications where meshes must be
distributed across thousands of MPI ranks. Report D1.2 provides some details of the preliminary studies we carried over for this integration.

The main features of the new function are:
\begin{itemize}
  \item Support for 1D, 2D, and 3D meshes partitioned via Gmsh's built-in
        partitioner (e.g.\ using the command line flags
        \verb|-part N -part_ghosts -part_split|).
  \item Automatic handling of ghost cell identification, vertex coordinates,
        and cell connectivity across MPI processes.
\end{itemize}

This enhancement enables scalable parallel mesh handling and substantially
simplifies the workflow for distributed simulations using deal.II and Gmsh.
By allowing Gmsh to act as a pre-partitioner for distributed meshes, the new
functionality removes the need for expensive repartitioning inside deal.II,
and integrates seamlessly with adaptive and fully distributed solvers.  

The pull request has reached a mature stage and is expected to be merged into
deal.II in the coming release cycle, completing the Gmsh-related part of the
dealii-X roadmap. This work directly addresses the bottleneck of mesh generation
and partitioning for realistic biomedical geometries, such as brain and liver
models, in exascale digital twin applications.


\subsection{Development of block preconditioners (WP1.2)}
\label{sec:preconditioning}
This section describes the developments of novel block preconditioners for the solution of problems stemming from
non-matching discretizations and fictitious domain approaches, which are relevant in the solution
of fluid-structure interaction problems. In this context, the use of Lagrange multipliers to enforce coupling conditions
leads to the solution of large (and multiple) saddle point linear systems, for which the design of robust and
efficient preconditioners is still a challenge.

We started our investigations with Poisson and Stokes interface problems, where the interface conditions are enforced
using Lagrange multipliers. In particular, our preconditioning strategy is based on augmented Lagrangian ideas~\cite{BenziAL}, where the original saddle point system is recasted
as an equivalent one, which provides better spectral properties. The resulting linear systems are solved using a Krylov subspace method such as (flexible) GMRES.

A thorough theoretical analysis of the preconditioner has been carried out, providing lower and upper bounds for the spectrum of
the preconditioned system and demonstrating independence with respect to the mesh size. Several numerical experiments have been conducted,
in order to validate the theoretical findings and to assess the performance of the preconditioner. The associated work is currently under revision, and a preprint is available on
arXiv~\cite{ALprec}.

A memory-distributed implementation based on \texttt{deal.II} has been developed and is available at the following maintained GitHub
repository \url{https://github.com/fdrmrc/fictitious_domain_AL_preconditioners}.

Current efforts are directed on the extension of the proposed preconditioning technique to handle
the following more complex scenarios:
\begin{itemize}
    \item Elliptic interface problems with large jumps in the coefficients.
    \item Fluid-structure interaction problems.
\end{itemize}
Preliminary results indicate that the preconditioner maintains its robustness and efficiency in these more
challenging settings.

\begin{thebibliography}{10}
    \bibitem{BenziAL} M. Benzi, and M. Olshanskii "An Augmented Lagrangian-Based Approach to the Oseen Problem", SIAM Journal on Scientific
Computing, vol. 28, no. 6, pp. 2095-2113, 2006.
    \bibitem{ALprec} M. Benzi, M. Feder, L. Heltai, and F. Mugnaioni "Scalable augmented Lagrangian preconditioners for fictitious domain problems," \url{https://arxiv.org/abs/2504.11339}, 2025.
\end{thebibliography}


\subsection{VTK interoperability: mesh \& field I/O utilities (prototype)}
Preliminary VTK support has been prototyped in the \texttt{reduced\_lagrange\_multipliers}
project (\url{https://github.com/luca-heltai/reduced_lagrange_multipliers/pull/40}),
providing a lightweight header (\texttt{vtk\_utils.h}) with utilities that bridge
VTK files and deal.II data structures (guarded by \texttt{DEAL\_II\_WITH\_VTK}).

\paragraph{Field readers (serial input).}
\begin{itemize}
  \item \texttt{read\_cell\_data(vtk\_filename, cell\_data\_name, Vector<double>\&):}
        reads a named \emph{cell} data array (scalar or vector) from a VTK
        \texttt{UnstructuredGrid} into a flat vector, in row-major order
        (cell\(_0\)\_comp\(_0\), \dots, cell\(_1\)\_comp\(_0\), \dots).
  \item \texttt{read\_vertex\_data(vtk\_filename, vertex\_data\_name, Vector<double>\&):}
        same for \emph{point/vertex} data arrays.
  \item \texttt{read\_data(vtk\_filename, Vector<double>\&):}
        concatenates all point data first (all fields, all components), then
        all cell data, in discovery order; useful as a one-shot bulk import.
\end{itemize}

\paragraph{Mesh readers and FE mapping.}
\begin{itemize}
  \item \texttt{read\_vtk(vtk\_filename, Triangulation\&):}
        imports a VTK mesh into a (serial) \texttt{Triangulation}; optional
        cleaning can merge overlapping points.
  \item \texttt{vtk\_to\_finite\_element(vtk\_filename):}
        constructs a suitable \texttt{FiniteElement} (returned as a
        \texttt{FESystem}) by inspecting VTK fields:
        point data $\rightarrow$ \texttt{FE\_Q} (or \texttt{FESystem(FE\_Q, n\_comps)}),
        cell data $\rightarrow$ \texttt{FE\_DGQ} (or \texttt{FESystem(FE\_DGQ, n\_comps)}).
        Also returns the vector of field names.
  \item \texttt{get\_block\_indices(fe):}
        returns a \texttt{BlockIndices} mapping for the FE (workaround for a 9.6 issue).
\end{itemize}

\paragraph{Data plumbing into DoFs (serial and distributed).}
\begin{itemize}
  \item \texttt{read\_vtk(vtk\_filename, DoFHandler\&, Vector<double>\&, std::vector<std::string>\&):}
        end-to-end routine that reads the mesh, builds the FE (as above),
        distributes DoFs block-wise, and fills a single data vector with all
        fields; also returns field names.
  \item \texttt{data\_to\_dealii\_vector(serial\_tria, data, dh, output\_vector):}
        maps bulk VTK data (\texttt{read\_data}) onto a (serial or parallel)
        \texttt{DoFHandler} vector of an FE built via \texttt{vtk\_to\_finite\_element}.
        It handles vertex- vs.\ cell-based fields with separate block offsets,
        checks ownership via \texttt{locally\_owned\_dofs}, and assumes serial
        and parallel meshes preserve cell ordering for cell fields.
  \item \texttt{serial\_vector\_to\_distributed\_vector(serial\_dh, parallel\_dh, serial\_vec, distributed\_vec):}
        transfers per-vertex (and per-cell) data from a serial vector to a
        distributed one.
  \item \texttt{distributed\_to\_serial\_vertex\_indices(serial\_tria, parallel\_tria):}
        builds a map from parallel vertex indices to serial indices for
        locally-owned vertices; used internally for consistent vertex data scatter.
\end{itemize}

\paragraph{Impact.} These utilities establish a reproducible VTK $\leftrightarrow$
deal.II path for both mesh and multi-field data, enabling:
(i) robust import of external unstructured grids,
(ii) automatic FE layout matching field arity (point vs.\ cell, scalar vs.\ vector),
(iii) consistent serial-to-distributed data transfer.
This reduces bespoke conversion scripts and supports large-scale workflows with
ParaView/VTK pipelines—crucial for pre-exascale digital twin scenarios.


\section{Polygonal Discretization Methods (WP1.3)}

The Polygonal discretization module has been further developed, enriched with a suite of test cases and benchmark applications, as reported in D1.5. Work is ongoing to merge the module into the official deal.II repository, including comprehensive documentation and tutorials.

\section{Integration of PSCToolkit (WP1.4)}

Release 9.7 includes preliminary hooks for the \texttt{PSBLAS}/PSCToolkit
linear algebra stack. This allows deal.II applications to leverage algebraic
multigrid preconditioners and GPU-enabled sparse BLAS routines. The main
advances are:

\begin{itemize}
  \item Configurable support for PSCToolkit backends through CMake, alongside
        existing PETSc and Trilinos.
  \item A common interface design consistent with deal.II’s existing solver
        wrappers, easing migration of application codes.
  \item Planned co-design with AMG4PSBLAS smoothers to support nonlinear and
        time-dependent biomedical solvers where frequent coarse-grid rebuilds
        are required.
\end{itemize}

This step is fundamental to enable hybrid matrix-free/matrix-based solvers
where PSCToolkit provides scalable algebraic coarse solvers complementing
deal.II’s matrix-free fine-scale kernels.

\section{Integration of MUMPS (WP1.5)}
\label{sec:section3}

The 9.7 release has introduced a direct wrapper for MUMPS, no longer requiring
integration solely through PETSc or Trilinos. The enhancements include:

\begin{itemize}
  \item Transparent support for parallel multifrontal factorization and
        out-of-core capabilities.
  \item Interfaces for block low-rank compression and mixed-precision
        algorithms, in line with recent MUMPS developments for exascale
        readiness.
  \item GPU acceleration for parts of the factorization, providing robustness in
        scenarios with ill-conditioned coarse problems such as multiphysics
        brain–liver coupling.
\end{itemize}

The integration of MUMPS as a first-class backend within deal.II ensures that
robust direct solvers are available when iterative approaches are insufficient.
This is particularly relevant for coarse-grid solvers in multigrid hierarchies,
and for nonlinear inversion tasks common in digital twin calibration.

\subsection{Challenges and Future Plans}

Despite these advances, several challenges remain:

\begin{itemize}
  \item Ensuring uniform GPU performance across NVIDIA, AMD, and Intel hardware
        remains a priority; AMD/Intel kernels are not yet tuned to the same
        level as CUDA-based ones.
  \item Minimization of data movement between CPU and GPU during assembly phases
        is ongoing, as deal.II assembly is still CPU-only in most workflows.
  \item Interoperability between polygonal discretizations, PSCToolkit AMG
        solvers, and MUMPS direct solvers must be systematically validated in
        multiphysics organ applications.
\end{itemize}

Future work in WP1 will finalize the integration of polygonal discretization
into mainline deal.II, extend GPU assembly kernels, and deliver comprehensive
benchmarks on EuroHPC systems (LEONARDO, LUMI, JUPITER). Training materials and
new tutorials will further disseminate exascale-ready modules to the broader
user community.

\label{MyLastPage}

\end{document}
